{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your assignment for this course is something similar: build a Python function that can take the file data/corpus.txt (UTF-8 encoded) from this repo as an argument and print a count of the 100 most frequent 1-grams (i.e. single words).\n",
    "\n",
    "In essence the job is to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 11852), ('', 5952), ('of', 5768), ('and', 5264), ('to', 4027), ('a', 3980), ('in', 3548), ('that', 2336), ('his', 2061), ('it', 1517), ('as', 1490), ('i', 1488), ('with', 1460), ('he', 1448), ('is', 1400), ('was', 1393), ('for', 1337), ('but', 1319), ('all', 1148), ('at', 1116), ('this', 1063), ('by', 1042), ('from', 944), ('not', 933), ('be', 863), ('on', 850), ('so', 763), ('you', 718), ('one', 694), ('have', 658), ('had', 647), ('or', 638), ('were', 551), ('they', 547), ('are', 504), ('some', 498), ('my', 484), ('him', 480), ('which', 478), ('their', 478), ('upon', 475), ('an', 473), ('like', 470), ('when', 458), ('whale', 456), ('into', 452), ('now', 437), ('there', 415), ('no', 414), ('what', 413), ('if', 404), ('out', 397), ('up', 380), ('we', 379), ('old', 365), ('would', 350), ('more', 348), ('been', 338), ('over', 324), ('only', 322), ('then', 312), ('its', 307), ('such', 307), ('me', 307), ('other', 301), ('will', 300), ('these', 299), ('down', 270), ('any', 269), ('than', 262), ('has', 257), ('very', 252), ('though', 245), ('yet', 245), ('those', 242), ('must', 238), ('them', 237), ('her', 237), ('do', 234), ('about', 234), ('said', 233), ('ye', 232), ('who', 231), ('still', 229), ('great', 229), ('most', 228), ('man', 220), ('two', 219), ('seemed', 216), ('long', 214), ('your', 213), ('before', 212), ('it,', 210), ('thou', 210), ('ship', 209), ('after', 208), ('white', 207), ('did', 202), ('little', 201), ('him,', 194)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "def onegrams(file):\n",
    "    with open(file, 'r') as corpus:\n",
    "        text = corpus.read()\n",
    "        # .casefold() is better than .lower() here\n",
    "        # https://www.programiz.com/python-programming/methods/string/casefold\n",
    "        normalize = text.casefold()\n",
    "        words = normalize.split(' ')\n",
    "        count = Counter(words) \n",
    "        return count\n",
    "\n",
    "ngram_viewer = onegrams(os.path.join('data', 'corpus.txt'))\n",
    "print(ngram_viewer.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a twist: you can’t use the collections library…\n",
    "\n",
    "Moreover, try to think about what else may be suboptimal in this example. For instance, in this code all of the text is loaded into memory in one time (with the read() method). What would happen if we tried this on a really big text file?\n",
    "\n",
    "Most importantly, the count is also wrong. Check by counting in an editor, for instance, and try to find out why.\n",
    "\n",
    "If this is an easy task for you, you can also think about the graphical representation of the 1-gram count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14400, 'the'), (6645, 'of'), (6403, 'and'), (4666, 'a'), (4642, 'to'), (4161, 'in'), (2950, 'that'), (2520, 'his'), (2388, 'it'), (1944, 'i'), (1781, 'but'), (1747, 'he'), (1728, 'is'), (1728, 'as'), (1727, 'with'), (1638, 'was'), (1623, 'for'), (1485, 'all'), (1405, 'this'), (1323, 'at'), (1225, 'by'), (1141, 'not'), (1102, 'from'), (1065, 'on'), (1050, 'so'), (1050, 'be'), (1044, 'him'), (956, 'whale'), (916, 'you'), (901, 'one'), (777, 'had'), (770, 'have'), (767, 'there'), (766, 'now'), (758, 'or'), (682, 'were'), (659, 'they'), (642, 'which'), (619, 'their'), (618, 'then'), (617, 'some'), (610, 'me'), (608, 'are'), (598, 'when'), (593, 'an'), (586, 'my'), (577, 'no'), (576, 'like'), (565, 'upon'), (545, 'what'), (524, 'into'), (520, 'out'), (509, 'up'), (509, 'more'), (497, 'if'), (465, 'its'), (459, 'them'), (457, 'we'), (440, 'old'), (435, 'man'), (428, 'would'), (421, 'ye'), (417, 'ahab'), (416, 'other'), (415, 'been'), (405, 'over'), (403, 'these'), (391, 'will'), (389, 'whales'), (383, 'ship'), (379, 'sea'), (377, 'only'), (372, 'such'), (364, 'though'), (361, 'down'), (341, 'any'), (339, 'yet'), (334, 'who'), (328, 'time'), (323, 'her'), (320, 'very'), (316, 'long'), (312, 'about'), (310, 'than'), (308, 'still'), (307, 'those'), (307, 'do'), (303, 'great'), (297, 'before'), (294, 'captain'), (293, 'said'), (293, 'has'), (290, 'here'), (284, 'two'), (283, 'must'), (282, 'most'), (281, 'seemed'), (275, 'last'), (269, 'head'), (264, 'thou')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import operator, string\n",
    "\n",
    "\n",
    "def ngram_counter(file):\n",
    "    with open(file, 'r', encoding ='utf-8') as corpus:\n",
    "        word_dic = {}\n",
    "        while True:\n",
    "            line= corpus.readline()\n",
    "            if line:\n",
    "                normalize = line.casefold()\n",
    "                strip_punct = normalize.translate(normalize.maketrans(\"\",\"\", string.punctuation))\n",
    "                words = strip_punct.split()\n",
    "                for word in words:\n",
    "                    if word in word_dic:\n",
    "                        word_dic[word] += 1\n",
    "                    else:\n",
    "                        word_dic[word] = 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "    return word_dic\n",
    "                \n",
    "   \n",
    "   \n",
    "    \n",
    "ngram_counter = ngram_counter(os.path.join('data', 'corpus.txt'))\n",
    "sorted_ngram = sorted(((value, key) for (key,value) in ngram_counter.items()), reverse = True)\n",
    "print(sorted_ngram[:100])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
